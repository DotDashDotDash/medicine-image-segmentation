## 激活函数

### 为什么需要激活函数

我们知道生物神经网络启发了人工神经网络的发展。但是，ANN 并非大脑运作的近似表示。所以在我们了解为什么在人工神经网络中使用激活函数之前，我们需要先了解一下生物神经网络与激活函数的相关性。

典型神经元的物理结构包括细胞体（cell body）、向其他神经元发送信号的轴突（axon）和接收其他神经元发送的信号或信息的树突（dendrites）。如下图所示:

<div align=center><img src="/assets/pics/j1.jpg"/></div>

上图中，红色圆圈代表两个神经元交流的区域。神经元通过树突接收来自其他神经元的信号。树突的权重叫作突触权值（synaptic weight），将和接收的信号相乘。来自树突的信号在细胞体内不断累积，如果信号强度超过特定阈值，则神经元向轴突传递信息。如未超过，则信号被该神经元「杀死」，无法进一步传播。

**激活函数决定是否传递信号**。在这种情况下，只需要带有一个参数（阈值）的简单阶梯函数。现在，当我们学习了一些新的东西（或未学习到什么）时，一些神经元的阈值和突触权值会发生改变。这使得神经元之间产生新的连接，大脑学会新的东西

### 神经网络

我们有必要对神经网络如何学习有一个基本了解。假设网络的期望输出是 y（标注值），但网络实际输出的是 y’（预测值）。预测输出和期望输出之间的差距（y - y’）可以转化成一种度量，即损失函数（J）。神经网络犯大量错误时，损失很高；神经网络犯错较少时，损失较低。训练目标就是找到使训练集上的损失函数最小化的权重矩阵和偏置向量。

在下图中，损失函数的形状像一个碗。在训练过程的任一点上，损失函数关于梯度的偏导数是那个位置的梯度。沿偏导数预测的方向移动，就可以到达谷底，使损失函数最小化。使用函数的偏导数迭代地寻找局部极小值的方法叫作梯度下降。

<div align=center><img src="/assets/pics/s1.jpg"/></div>

人工神经网络中的权重使用反向传播的方法进行更新。损失函数关于梯度的偏导数也用于更新权重。从某种意义上来说，神经网络中的误差根据求导的链式法则执行反向传播。这通过迭代的方式来实施，经过多次迭代后，损失函数达到极小值，其导数变为0

### 线性激活函数

这是一种简单的线性函数，公式为：f(x) = x。基本上，输入到输出过程中不经过修改。如下图所示：

<div align=center><img src="/assets/pics/l1.png"/></div>

### 为什么需要非线性激活函数

神经网络用于实现复杂的函数，非线性激活函数可以使神经网络随意逼近复杂函数。没有激活函数带来的非线性，多层神经网络和单层无异。

现在我们来看一个简单的例子，帮助我们了解为什么没有非线性，神经网络甚至无法逼近异或门（XOR gate）、同或门（XNOR gate）等简单函数。下图是一个异或门函数。叉和圈代表了数据集的两个类别。当 x_1、x_2 两个特征一样时，类别标签是红叉；不一样，就是蓝圈。两个红叉对于输入值 (0,0) 和 (1,1) 都有输出值 0，两个蓝圈对于输入值 (0,1) 和 (1,0) 都有输出值 1。

<div align=center><img src="/assets/pics/f1.png"/></div>

异或门函数

从上图中，我们可以看到数据点非线性可分。也就是说，我们无法画出一条直线使蓝圈和红叉分开来。因此，我们需要一个非线性决策边界（non-linear decision boundary）来分离它们。

激活函数对于将神经网络的输出压缩进特定边界内也非常关键。神经元的输出值可以非常大。该输出在未经修改的情况下馈送至下一层神经元时，可以被转换成更大的值，这样过程就需要极大算力。激活函数的一个任务就是将神经元的输出映射到有界的区域（如，0 到 1 之间）。

了解这些背景知识之后，我们就可以了解不同类型的激活函数了

### Sigmoid激活函数

<div align=center><img src="/assets/pics/s2.png"/></div>

Sigmoid 函数的三个主要缺陷：

* 梯度消失：注意：Sigmoid 函数趋近 0 和 1 的时候变化率会变得平坦，也就是说，Sigmoid 的梯度趋近于 0。神经网络使用 Sigmoid 激活函数进行反向传播时，输出接近 0 或 1 的神经元其梯度趋近于 0。这些神经元叫作饱和神经元。因此，这些神经元的权重不会更新。此外，与此类神经元相连的神经元的权重也更新得很慢。该问题叫作梯度消失。因此，想象一下，如果一个大型神经网络包含 Sigmoid 神经元，而其中很多个都处于饱和状态，那么该网络无法执行反向传播。
* 不以零为中心：Sigmoid 输出不以零为中心的。
* 计算成本高昂：exp() 函数与其他非线性激活函数相比，计算成本高昂。

### Tanh激活函数

<div align=center><img src="/assets/pics/t1.png"/></div>

Tanh导数

Tanh 激活函数又叫作双曲正切激活函数（hyperbolic tangent activation function）。与 Sigmoid 函数类似，Tanh 函数也使用真值，但 Tanh 函数将其压缩至-1 到 1 的区间内。与 Sigmoid 不同，Tanh 函数的输出以零为中心，因为区间在-1 到 1 之间。你可以将 Tanh 函数想象成两个 Sigmoid 函数放在一起。在实践中，Tanh 函数的使用优先性高于 Sigmoid 函数。负数输入被当作负值，零输入值的映射接近零，正数输入被当作正值。唯一的缺点是：

**Tanh 函数也会有梯度消失的问题，因此在饱和时也会「杀死」梯度**

为了解决梯度消失问题，我们来讨论另一个非线性激活函数——修正线性单元（rectified linear unit，ReLU），该函数明显优于前面两个函数，是现在使用最广泛的函数

### ReLU激活函数

<div align=center><img src="/assets/pics/r1.png"/><div>

